import pandas as pd
import numpy as np
import requests
import io
import logging
import joblib 

# ML and Preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline, make_pipeline # Import make_pipeline
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
import lightgbm as lgb

# Explainability
import dice_ml

# Fairness
from fairlearn.metrics import MetricFrame, count, equalized_odds_difference

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

DATASET_URL = "https://raw.githubusercontent.com/SasinduChanakaPiyumal/Loan-Prediction-Model/refs/heads/main/loan_data_set.csv"
TARGET_COLUMN = 'Loan_Status'
SENSITIVE_FEATURE = 'Gender' 
PROTECTED_GROUP_VALUE = 'Female' 
TARGET_MAP = {'Y': 0, 'N': 1} # 0 = Approved, 1 = Rejected

N_CFS_PER_INSTANCE = 5   
MAX_SYNTHETIC_SAMPLES = 200 
CF_COST_THRESHOLD = 3     

# --- Helper Functions --- (load_data remains the same)
def load_data(url):
    # ... (same as before) ...
    logging.info(f"Attempting to load data from: {url}")
    try:
        response = requests.get(url)
        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
        df = pd.read_csv(io.StringIO(response.text))
        logging.info(f"Data loaded successfully. Shape: {df.shape}")
        return df
    except requests.exceptions.RequestException as e:
        logging.error(f"Error downloading data: {e}")
        return None
    except pd.errors.ParserError as e:
        logging.error(f"Error parsing CSV data: {e}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred during data loading: {e}")
        return None


def create_preprocessor(X_train_sample):
    """Creates the ColumnTransformer based on a sample of the training data."""
    logging.info("Creating preprocessor...")
    
    # Identify feature types from the sample
    numerical_features = X_train_sample.select_dtypes(include=np.number).columns.tolist()
    categorical_features = X_train_sample.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # Handle 'Dependents' specifically if needed (based on previous analysis)
    if 'Dependents' in X_train_sample.columns and X_train_sample['Dependents'].dtype == 'object':
         if 'Dependents' not in numerical_features and 'Dependents' in categorical_features:
             categorical_features.remove('Dependents') # Remove from cat if present
             numerical_features.append('Dependents') # Add to num

    # Adjust for sensitive feature if needed
    if SENSITIVE_FEATURE in numerical_features and SENSITIVE_FEATURE not in categorical_features:
        numerical_features.remove(SENSITIVE_FEATURE)
        categorical_features.append(SENSITIVE_FEATURE)

    logging.info(f"Preprocessor using Numerical features: {numerical_features}")
    logging.info(f"Preprocessor using Categorical features: {categorical_features}")
    
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) 
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ],
        remainder='passthrough' 
    )
    return preprocessor, numerical_features, categorical_features

# calculate_metrics remains the same
def calculate_metrics(y_true, y_pred, s_attr, model_name="Model"):
    # ... (same as before) ...
    accuracy = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred) # Assumes positive label is 1 (Rejected)
    try:
        auc = roc_auc_score(y_true, y_pred) # Requires probabilities for true AUC, using binary here
    except ValueError:
        auc = np.nan # Handle cases where only one class is present in y_true or y_pred

    metrics = {'accuracy': accuracy_score, 'f1': f1_score, 'count': count}
    try:
        grouped_on_sex = MetricFrame(metrics=metrics,
                                     y_true=y_true,
                                     y_pred=y_pred,
                                     sensitive_features=s_attr)
        eq_odds_diff = equalized_odds_difference(y_true, y_pred, sensitive_features=s_attr)
        logging.info(f"Metrics By Group ({SENSITIVE_FEATURE}):\n{grouped_on_sex.by_group}")
    except Exception as e:
        logging.warning(f"Could not calculate grouped metrics: {e}")
        eq_odds_diff = np.nan

    logging.info(f"--- Metrics for {model_name} ---")
    logging.info(f"Accuracy: {accuracy:.4f}")
    logging.info(f"F1 Score: {f1:.4f}")
    logging.info(f"AUC (approx): {auc:.4f}")
    logging.info(f"Equalized Odds Difference: {eq_odds_diff:.4f}")


    return {"Accuracy": accuracy, "F1": f1, "AUC": auc, "EqualizedOddsDiff": eq_odds_diff}
# --- Main Execution Logic ---

# 1. Load Data
df_raw = load_data(DATASET_URL)
if df_raw is None: exit()

# 2. Basic Cleaning (Target mapping, ID drop)
logging.info("Performing basic cleaning...")
df = df_raw.drop(columns=['Loan_ID'], errors='ignore')
if TARGET_COLUMN not in df.columns:
    logging.error(f"Target column '{TARGET_COLUMN}' not found.")
    exit()
df[TARGET_COLUMN] = df[TARGET_COLUMN].map(TARGET_MAP)
df = df.dropna(subset=[TARGET_COLUMN])
df[TARGET_COLUMN] = df[TARGET_COLUMN].astype(int)
# Convert 'Dependents' early if object type
if 'Dependents' in df.columns and df['Dependents'].dtype == 'object':
    df['Dependents'] = df['Dependents'].replace('3+', '3').astype(float)

# 3. Split Data (Raw features, needed for DiCE later)
logging.info("Splitting data into train/test...")
y = df[TARGET_COLUMN]
X = df.drop(columns=[TARGET_COLUMN])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
# Separate sensitive features
s_train = X_train[SENSITIVE_FEATURE].copy().fillna('Unknown').astype(str)
s_test = X_test[SENSITIVE_FEATURE].copy().fillna('Unknown').astype(str)

# 4. Create and Fit Preprocessor
preprocessor, numerical_features, categorical_features = create_preprocessor(X_train)
logging.info("Fitting preprocessor on training data...")
preprocessor.fit(X_train) # Fit on the raw training data

# 5. Create Full ML Pipeline (Preprocessor + Model)
logging.info("--- Training Baseline Model (LightGBM Pipeline) ---")
baseline_model_clf = lgb.LGBMClassifier(random_state=42, objective='binary')
baseline_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                                  ('classifier', baseline_model_clf)])

baseline_pipeline.fit(X_train, y_train) # Train pipeline on raw features

# 6. Evaluate Baseline Model
logging.info("--- Evaluating Baseline Model ---")
y_pred_baseline = baseline_pipeline.predict(X_test) # Predict on raw test features
baseline_metrics = calculate_metrics(y_test, y_pred_baseline, s_test, "Baseline LGBM Pipeline")

# 7. Counterfactual Generation Setup
logging.info("--- Setting up Counterfactual Explainer (DiCE) ---")

# IMPORTANT: Initialize DiCE with UNPROCESSED training data and lists of original feature types
# DiCE handles the transformation using the provided model (which should be a pipeline)
dice_data = dice_ml.Data(dataframe=X_train.assign(**{TARGET_COLUMN: y_train}), # Use original X_train + y_train
                           continuous_features=numerical_features, # Original continuous list
                           outcome_name=TARGET_COLUMN)

# Pass the FULL PIPELINE to DiCE
dice_model = dice_ml.Model(model=baseline_pipeline, backend="sklearn") # Pass the pipeline!

# Initialize DiCE explainer
dice_explainer = dice_ml.Dice(dice_data, dice_model, method="random")
logging.info("DiCE Explainer Initialized.")

# 8. Identify Instances and Generate CFs
logging.info("--- Generating Counterfactuals for Rejected Protected Group ---")
# Identify protected group indices in the original training set
protected_group_indices = X_train.index[s_train == PROTECTED_GROUP_VALUE]

# Identify instances predicted as REJECTED (1) by the PIPELINE within the protected group
X_train_protected = X_train.loc[protected_group_indices]
y_train_protected_true = y_train.loc[protected_group_indices]
# y_pred_baseline_train_protected = baseline_pipeline.predict(X_train_protected) # Predict on raw

# Focus on TRUE rejections within the protected group
instances_to_explain_idx = X_train_protected.index[y_train_protected_true == 1]
instances_to_explain_df = X_train.loc[instances_to_explain_idx] # Get the raw feature rows
logging.info(f"Identified {len(instances_to_explain_df)} instances from protected group (rejected) to explain.")

fair_cfs_list = []
num_explained = 0
if not instances_to_explain_df.empty:
    for i in range(len(instances_to_explain_df)):
        # Pass the RAW instance to DiCE
        instance_raw = instances_to_explain_df.iloc[[i]]
        try:
            # Generate CFs aiming for Approval (0)
            cfs = dice_explainer.generate_counterfactuals(instance_raw,
                                                          total_CFs=N_CFS_PER_INSTANCE,
                                                          desired_class=0) # 0 = Approved

            if cfs and cfs.cf_examples_list and cfs.cf_examples_list[0].final_cfs_df is not None:
                 num_explained += 1
                 # Analyze CFs (still uses the raw features)
                 original_instance_values = instance_raw.iloc[0]
                 for cf_index, cf_row in cfs.cf_examples_list[0].final_cfs_df.iterrows():
                     # Calculate cost (number of features changed in the *original* feature space)
                     changed_features = (original_instance_values != cf_row[:-1]).sum() # Exclude target
                     if changed_features <= CF_COST_THRESHOLD:
                         # Store the features of the "fair" counterfactual (still in original format)
                         fair_cfs_list.append(cf_row[:-1]) # Exclude target column
                 if num_explained % 20 == 0: # Log progress
                    logging.info(f"Generated/Analyzed CFs for {num_explained} instances...")

        except Exception as e:
            logging.warning(f"DiCE failed for instance index {instance_raw.index[0]}: {e}", exc_info=False) # Set exc_info=False for cleaner logs
            continue
else:
     logging.warning("No instances found matching criteria for counterfactual explanation.")


logging.info(f"Generated CFs for {num_explained} instances.")
logging.info(f"Found {len(fair_cfs_list)} 'fair' counterfactuals meeting cost threshold <={CF_COST_THRESHOLD}.")

# 9. Augment Data
if fair_cfs_list:
    if len(fair_cfs_list) > MAX_SYNTHETIC_SAMPLES:
        logging.info(f"Selecting {MAX_SYNTHETIC_SAMPLES} from {len(fair_cfs_list)} fair CFs.")
        indices = np.random.choice(len(fair_cfs_list), MAX_SYNTHETIC_SAMPLES, replace=False)
        selected_cfs = [fair_cfs_list[i] for i in indices]
    else:
        selected_cfs = fair_cfs_list

    # Create DataFrame from selected CFs (still in original feature format)
    synthetic_df = pd.DataFrame(selected_cfs, columns=X_train.columns)

    # Create synthetic target (all should be Approved = 0)
    synthetic_y = pd.Series([0] * len(synthetic_df), name=TARGET_COLUMN)

    # Combine original and synthetic data (still in original format)
    X_train_augmented = pd.concat([X_train, synthetic_df], ignore_index=True)
    y_train_augmented = pd.concat([y_train, synthetic_y], ignore_index=True)

    logging.info(f"Augmented training data. New shape: {X_train_augmented.shape}")

    # 10. Retrain Model (Train the *pipeline* on the augmented *raw* data)
    logging.info("--- Retraining Model on Augmented Data ---")
    fair_model_clf = lgb.LGBMClassifier(random_state=42, objective='binary') # New classifier instance
    fair_pipeline = Pipeline(steps=[('preprocessor', preprocessor), # Use the *same fitted* preprocessor
                                  ('classifier', fair_model_clf)])
    # Retrain the pipeline on the augmented raw data
    fair_pipeline.fit(X_train_augmented, y_train_augmented)

    # 11. Evaluate Retrained Model
    logging.info("--- Evaluating Retrained (Fairer) Model ---")
    y_pred_fair = fair_pipeline.predict(X_test) # Predict on raw test features
    retrained_metrics = calculate_metrics(y_test, y_pred_fair, s_test, "Retrained LGBM Pipeline (FAAC)")

    # 12. Compare Metrics
    logging.info("--- Comparison ---")
    logging.info(f"Fairness (EqOddsDiff): Baseline={baseline_metrics['EqualizedOddsDiff']:.4f}, Retrained={retrained_metrics['EqualizedOddsDiff']:.4f}")
    logging.info(f"Accuracy: Baseline={baseline_metrics['Accuracy']:.4f}, Retrained={retrained_metrics['Accuracy']:.4f}")
    logging.info(f"F1 Score: Baseline={baseline_metrics['F1']:.4f}, Retrained={retrained_metrics['F1']:.4f}")

else:
    logging.warning("No fair counterfactuals generated, skipping augmentation and retraining.")

logging.info("--- Script Finished ---")